{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-16T18:18:50.158539Z",
     "iopub.status.busy": "2024-03-16T18:18:50.158141Z",
     "iopub.status.idle": "2024-03-16T18:19:24.624326Z",
     "shell.execute_reply": "2024-03-16T18:19:24.623173Z",
     "shell.execute_reply.started": "2024-03-16T18:18:50.158499Z"
    }
   },
   "outputs": [],
   "source": [
    "# install the followinf libraries -q -U accelerate=='0.25.0' peft=='0.7.1' bitsandbytes=='0.41.3.post2' transformers=='4.36.1' trl=='0.7.4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-16T18:19:24.626720Z",
     "iopub.status.busy": "2024-03-16T18:19:24.626398Z",
     "iopub.status.idle": "2024-03-16T18:19:24.633544Z",
     "shell.execute_reply": "2024-03-16T18:19:24.632415Z",
     "shell.execute_reply.started": "2024-03-16T18:19:24.626681Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-16T18:19:24.634911Z",
     "iopub.status.busy": "2024-03-16T18:19:24.634656Z",
     "iopub.status.idle": "2024-03-16T18:19:24.644553Z",
     "shell.execute_reply": "2024-03-16T18:19:24.643622Z",
     "shell.execute_reply.started": "2024-03-16T18:19:24.634888Z"
    }
   },
   "outputs": [],
   "source": [
    "# silence warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-16T18:19:24.646881Z",
     "iopub.status.busy": "2024-03-16T18:19:24.646635Z",
     "iopub.status.idle": "2024-03-16T18:19:57.093991Z",
     "shell.execute_reply": "2024-03-16T18:19:57.093266Z",
     "shell.execute_reply.started": "2024-03-16T18:19:24.646860Z"
    },
    "papermill": {
     "duration": 14.485002,
     "end_time": "2023-10-16T11:00:18.917449",
     "exception": false,
     "start_time": "2023-10-16T11:00:04.432447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-16T18:19:57.095468Z",
     "iopub.status.busy": "2024-03-16T18:19:57.095118Z",
     "iopub.status.idle": "2024-03-16T18:19:58.134225Z",
     "shell.execute_reply": "2024-03-16T18:19:58.133247Z",
     "shell.execute_reply.started": "2024-03-16T18:19:57.095436Z"
    }
   },
   "outputs": [],
   "source": [
    "# load training and test sets\n",
    "\n",
    "train = \"X_train_u_F.xlsx\"\n",
    "test = \"X_test_u_F.xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "train = pd.read_excel(train)\n",
    "test = pd.read_excel(test)\n",
    "\n",
    "X = train[\"patient medical hidtory\"]\n",
    "y = train[\"Inhospital Mortality\"]\n",
    "\n",
    "\n",
    "# seprate 0.2 of training set for evaluation\n",
    "X_train, X_eval, y_train, y_eval = train_test_split( X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "eval = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# preprocess and convert data into text\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            [INST]You're tasked with analyzing the present symptoms, past medical history, \n",
    "            laboratory data, age, and gender of COVID patients to determine their outcome, \n",
    "            which is enclosed in square brackets. Your goal is to predict whether the patient will \"survive\" or \"die\" based on this information.[/INST]\n",
    "\n",
    "            [{data_point[\"patient medical hidtory\"]}] = {data_point[\"Inhospital Mortality\"]}\n",
    "            \"\"\".strip()\n",
    "\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            [INST]You're tasked with analyzing the present symptoms, past medical history, \n",
    "            laboratory data, age, and gender of COVID patients to determine their outcome, \n",
    "            which is enclosed in square brackets. Your goal is to predict whether the patient will \"survive\" or \"die\" based on this information.[/INST]\n",
    "\n",
    "            [{data_point[\"patient medical hidtory\"]}] = \"\"\".strip()\n",
    "\n",
    "X_train = pd.DataFrame(train.apply(generate_prompt, axis=1),\n",
    "                       columns=[\"patient medical hidtory\"])\n",
    "X_eval = pd.DataFrame(eval.apply(generate_prompt, axis=1),\n",
    "                      columns=[\"patient medical hidtory\"])\n",
    "\n",
    "y_true = test[\"Inhospital Mortality\"]\n",
    "X_test = pd.DataFrame(test.apply(generate_test_prompt, axis=1), columns=[\"patient medical hidtory\"])\n",
    "\n",
    "train_data = Dataset.from_pandas(X_train)\n",
    "eval_data = Dataset.from_pandas(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-16T18:19:58.146823Z",
     "iopub.status.busy": "2024-03-16T18:19:58.146463Z",
     "iopub.status.idle": "2024-03-16T18:22:15.830626Z",
     "shell.execute_reply": "2024-03-16T18:22:15.829493Z",
     "shell.execute_reply.started": "2024-03-16T18:19:58.146788Z"
    }
   },
   "outputs": [],
   "source": [
    "# load model , tokenizer and set BitsAndBytesConfig\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          trust_remote_code=True,\n",
    "                                          padding_side=\"left\",\n",
    "                                          add_eos_token=True,\n",
    "                                         )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-16T18:22:15.832810Z",
     "iopub.status.busy": "2024-03-16T18:22:15.832302Z",
     "iopub.status.idle": "2024-03-16T18:22:15.841040Z",
     "shell.execute_reply": "2024-03-16T18:22:15.840126Z",
     "shell.execute_reply.started": "2024-03-16T18:22:15.832776Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to make a list of y predictions\n",
    "\n",
    "def predict(X_test, model, tokenizer):\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(X_test))):\n",
    "        prompt = X_test.iloc[i][\"patient medical hidtory\"]\n",
    "        pipe = pipeline(task=\"text-generation\",\n",
    "                        model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        max_new_tokens = 2,\n",
    "                        temperature = 0.0,\n",
    "                       )\n",
    "        result = pipe(prompt, pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "        answer = result[0]['generated_text'].split(\"=\")[-1].lower()\n",
    "        if \"die\" in answer:\n",
    "            y_pred.append(\"die\")\n",
    "        else:\n",
    "            y_pred.append(\"survive\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-16T18:22:15.842740Z",
     "iopub.status.busy": "2024-03-16T18:22:15.842457Z",
     "iopub.status.idle": "2024-03-16T18:30:28.347164Z",
     "shell.execute_reply": "2024-03-16T18:30:28.346235Z",
     "shell.execute_reply.started": "2024-03-16T18:22:15.842716Z"
    }
   },
   "outputs": [],
   "source": [
    "# mortality is predicted  by zero shot classification method\n",
    "\n",
    "y_pred = predict(X_test, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-16T18:30:28.717086Z",
     "iopub.status.busy": "2024-03-16T18:30:28.716694Z",
     "iopub.status.idle": "2024-03-16T18:30:28.750102Z",
     "shell.execute_reply": "2024-03-16T18:30:28.749201Z",
     "shell.execute_reply.started": "2024-03-16T18:30:28.717056Z"
    }
   },
   "outputs": [],
   "source": [
    "# save predictions to csv file\n",
    "\n",
    "evaluation = pd.DataFrame({'text': X_test[\"patient medical hidtory\"], \n",
    "                           'y_true':y_true, \n",
    "                           'y_pred': y_pred},\n",
    "                         )\n",
    "evaluation.to_csv(\"zero_shot_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set peft config and trainig arguments \n",
    "\n",
    "model.config.max_new_tokens = 2\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"logs\",\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4, \n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"patient medical hidtory\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    max_seq_length=2500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(\"trained-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fine-tuned model to predict test set\n",
    "\n",
    "y_pred = predict(X_test, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to csv file\n",
    "\n",
    "evaluation = pd.DataFrame({'text': X_test[\"text\"], \n",
    "                           'y_true':y_true, \n",
    "                           'y_pred': y_pred},\n",
    "                         )\n",
    "evaluation.to_csv(\"test_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 622510,
     "sourceId": 1192499,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4608915,
     "sourceId": 7857666,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
